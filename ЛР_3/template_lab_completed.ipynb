{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b50d2d",
   "metadata": {},
   "source": [
    "# Лабораторная работа: Реализация простой нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# --- Функции активации ---\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1.0, 0.0)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "    'relu': (relu, relu_derivative),\n",
    "    'tanh': (tanh, tanh_derivative)\n",
    "}\n",
    "\n",
    "# --- Класс NeuralNetwork ---\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dims, activation_funcs, learning_rate=0.01):\n",
    "        if len(layer_dims) < 2:\n",
    "            raise ValueError(\"Сеть должна иметь как минимум входной и выходной слои.\")\n",
    "        if len(activation_funcs) != len(layer_dims) - 1:\n",
    "            raise ValueError(\"Количество функций активации должно соответствовать количеству слоев (исключая входной).\")\n",
    "        self.num_layers = len(layer_dims)\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = {}\n",
    "        self.activation_funcs = {}\n",
    "        np.random.seed(42)\n",
    "        for l in range(1, self.num_layers):\n",
    "            if activation_funcs[l-1] == 'relu':\n",
    "                self.parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2. / layer_dims[l-1])\n",
    "            else:\n",
    "                limit = np.sqrt(6. / (layer_dims[l-1] + layer_dims[l]))\n",
    "                self.parameters[f'W{l}'] = np.random.uniform(-limit, limit, (layer_dims[l], layer_dims[l-1]))\n",
    "            self.parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "            activation_name = activation_funcs[l-1]\n",
    "            if activation_name not in ACTIVATIONS:\n",
    "                raise ValueError(f\"Неизвестная функция активации: {activation_name}\")\n",
    "            self.activation_funcs[l] = ACTIVATIONS[activation_name]\n",
    "\n",
    "    def _forward(self, X):\n",
    "        cache = {}\n",
    "        A = X\n",
    "        cache[\"A0\"] = A\n",
    "        for l in range(1, self.num_layers):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            activation_func, _ = self.activation_funcs[l]\n",
    "            A_prev = A\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            A = activation_func(Z)\n",
    "            cache[f'Z{l}'] = Z\n",
    "            cache[f'A{l}'] = A\n",
    "        A_last = A\n",
    "        return A_last, cache\n",
    "\n",
    "    def _compute_cost(self, A_last, Y):\n",
    "        m = Y.shape[1]\n",
    "        epsilon = 1e-8\n",
    "        cost = -(1.0/m) * np.sum(Y * np.log(A_last + epsilon) + (1 - Y) * np.log(1 - A_last + epsilon))\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "    def _backward(self, A_last, Y, cache):\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        L = self.num_layers - 1\n",
    "        epsilon = 1e-8\n",
    "        dA_last = - (Y / (A_last + epsilon) - (1 - Y) / (1 - A_last + epsilon))\n",
    "        Z_last = cache[f'Z{L}']\n",
    "        A_prev = cache[f'A{L-1}']\n",
    "        _, activation_derivative = self.activation_funcs[L]\n",
    "        dZ_last = dA_last * activation_derivative(Z_last)\n",
    "        grads[f'dW{L}'] = (1.0/m) * np.dot(dZ_last, A_prev.T)\n",
    "        grads[f'db{L}'] = (1.0/m) * np.sum(dZ_last, axis=1, keepdims=True)\n",
    "        W_last = self.parameters[f'W{L}']\n",
    "        dA_prev = np.dot(W_last.T, dZ_last)\n",
    "        for l in range(L-1, 0, -1):\n",
    "            Z = cache[f'Z{l}']\n",
    "            A_prev_l = cache[f'A{l-1}']\n",
    "            _, activation_derivative = self.activation_funcs[l]\n",
    "            dZ = dA_prev * activation_derivative(Z)\n",
    "            grads[f'dW{l}'] = (1.0/m) * np.dot(dZ, A_prev_l.T)\n",
    "            grads[f'db{l}'] = (1.0/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            W_current = self.parameters[f'W{l}']\n",
    "            dA_prev = np.dot(W_current.T, dZ)\n",
    "        return grads\n",
    "\n",
    "    def _update_parameters(self, grads):\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.parameters[f'W{l}'] = self.parameters[f'W{l}'] - self.learning_rate * grads[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] = self.parameters[f'b{l}'] - self.learning_rate * grads[f'db{l}']\n",
    "\n",
    "    def fit(self, X_train, Y_train, epochs, batch_size, print_cost_every=100):\n",
    "        costs = []\n",
    "        m = X_train.shape[1]\n",
    "        np.random.seed(1)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0.\n",
    "            permutation = np.random.permutation(m)\n",
    "            shuffled_X = X_train[:, permutation]\n",
    "            shuffled_Y = Y_train[:, permutation]\n",
    "            num_minibatches = m // batch_size\n",
    "            if m % batch_size != 0:\n",
    "                num_minibatches += 1\n",
    "            for i in range(num_minibatches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min(start_idx + batch_size, m)\n",
    "                mini_batch_X = shuffled_X[:, start_idx:end_idx]\n",
    "                mini_batch_Y = shuffled_Y[:, start_idx:end_idx]\n",
    "                A_last, cache = self._forward(mini_batch_X)\n",
    "                batch_cost = self._compute_cost(A_last, mini_batch_Y)\n",
    "                epoch_cost += batch_cost * mini_batch_X.shape[1]\n",
    "                grads = self._backward(A_last, mini_batch_Y, cache)\n",
    "                self._update_parameters(grads)\n",
    "            epoch_cost /= m\n",
    "            costs.append(epoch_cost)\n",
    "            if print_cost_every > 0 and (epoch % print_cost_every == 0 or epoch == epochs - 1):\n",
    "                print(f\"Эпоха {epoch}: стоимость = {epoch_cost:.6f}\")\n",
    "        return costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        A_last, _ = self._forward(X)\n",
    "        predictions = (A_last > 0.5).astype(int)\n",
    "        return predictions\n",
    "\n",
    "# --- Данные и обучение ---\n",
    "X, Y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train = X.T\n",
    "Y_train = Y.reshape(1, Y.shape[0])\n",
    "\n",
    "layer_dims = [X_train.shape[0], 5, 3, 1]\n",
    "activation_funcs = ['relu', 'relu', 'sigmoid']\n",
    "nn = NeuralNetwork(layer_dims, activation_funcs, learning_rate=0.1)\n",
    "costs = nn.fit(X_train, Y_train, epochs=2000, batch_size=64, print_cost_every=500)\n",
    "\n",
    "# --- График функции потерь ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(costs)\n",
    "plt.title(\"Функция потерь во время обучения\")\n",
    "plt.xlabel(\"Эпохи\")\n",
    "plt.ylabel(\"Стоимость\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Граница решений ---\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[0, :].min() - 0.5, X[0, :].max() + 0.5\n",
    "    y_min, y_max = X[1, :].min() - 0.5, X[1, :].max() + 0.5\n",
    "    h = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    grid_input = np.c_[xx.ravel(), yy.ravel()].T\n",
    "    Z = model.predict(grid_input)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[0, :], X[1, :], c=y.ravel(), cmap=plt.cm.Spectral, edgecolors='k')\n",
    "    plt.title(\"Граница решений нейронной сети\")\n",
    "    plt.xlabel(\"Признак 1\")\n",
    "    plt.ylabel(\"Признак 2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(nn, X_train, Y_train)\n",
    "\n",
    "preds = nn.predict(X_train)\n",
    "accuracy = np.mean(preds == Y_train) * 100\n",
    "print(f\"Точность на обучающем наборе: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
